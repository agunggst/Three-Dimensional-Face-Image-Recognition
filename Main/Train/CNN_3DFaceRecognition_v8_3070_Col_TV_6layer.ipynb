{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "#%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('Train_Raw_C3.mat')\n",
    "Train_Raw = mat['Train_Raw']\n",
    "mat1 = scipy.io.loadmat('Val_Raw_C3.mat')\n",
    "Val_Raw = mat1['Val_Raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "3750\n"
     ]
    }
   ],
   "source": [
    "l,w = 45,35\n",
    "Total_Train_Photo = int((Train_Raw.shape[0]*Train_Raw.shape[1])/(l*w))\n",
    "Total_Val_Photo = int((Val_Raw.shape[0]*Val_Raw.shape[1])/(l*w))\n",
    "Photo_Train_Col = 12\n",
    "Photo_Val_Col = 25\n",
    "print(Total_Train_Photo)\n",
    "print(Total_Val_Photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalisasi\n",
    "Train_Raw_Norm = Train_Raw/255\n",
    "Val_Raw_Norm = Val_Raw/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 45, 35)\n",
      "(3750, 45, 35)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "input_data_raw = np.zeros([Total_Train_Photo,l,w])\n",
    "val_data_raw = np.zeros([Total_Val_Photo,l,w])\n",
    "for a in range(0,int(Total_Train_Photo/Photo_Train_Col)):\n",
    "    for b in range(0,Photo_Train_Col):\n",
    "        input_data_raw[i,:,:] = Train_Raw_Norm[45*a:45+45*a,35*b:35+35*b]\n",
    "        i=i+1\n",
    "i,a,b=0,0,0\n",
    "for a in range(0,int(Total_Val_Photo/Photo_Val_Col)):\n",
    "    for b in range(0,Photo_Val_Col):\n",
    "        val_data_raw[i,:,:] = Val_Raw_Norm[45*a:45+45*a,35*b:35+35*b]\n",
    "        i=i+1\n",
    "        \n",
    "print(input_data_raw.shape)\n",
    "print(val_data_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(input_data_raw[1799,:,:], cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 10)\n",
      "(3750, 10)\n"
     ]
    }
   ],
   "source": [
    "tar = np.eye(10)\n",
    "target_train = np.zeros([Total_Train_Photo,10])\n",
    "target_val = np.zeros([Total_Val_Photo,10])\n",
    "for d in range(0,10):    \n",
    "    target_train[0+int(Total_Train_Photo/10)*d:int(Total_Train_Photo/10)+int(Total_Train_Photo/10)*d,:] = tar[d,:]\n",
    "d=0\n",
    "for d in range(0,10):\n",
    "    target_val[0+int(Total_Val_Photo/10)*d:int(Total_Val_Photo/10)+int(Total_Val_Photo/10)*d,:] = tar[d,:]\n",
    "print(target_train.shape)\n",
    "print(target_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tebal_train, tebal_val = Total_Train_Photo, Total_Val_Photo\n",
    "target_train3d = np.zeros(shape=(tebal_train, 1, 10))\n",
    "target_val3d = np.zeros(shape=(tebal_val, 1, 10))\n",
    "i=0\n",
    "\n",
    "for i in range(tebal_train):\n",
    "    target_train3d[i,0,:] = target_train[i,:]\n",
    "for i in range(tebal_val):\n",
    "    target_val3d[i,0,:] = target_val[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_in4d(data):\n",
    "    image_size = [45,35]\n",
    "    data4d = np.zeros([Total_Train_Photo,1,image_size[0],image_size[1]])\n",
    "    data4d[:,0,:,:] = data\n",
    "    return data4d\n",
    "\n",
    "def val_inl4d(data):\n",
    "    image_size = [45,35]\n",
    "    data4d = np.zeros([Total_Val_Photo,1,image_size[0],image_size[1]])\n",
    "    data4d[:,0,:,:] = data\n",
    "    return data4d\n",
    "\n",
    "def train_tar4d(data):\n",
    "    image_size = [1,10]\n",
    "    data4d = np.zeros([Total_Train_Photo,1,image_size[0],image_size[1]])\n",
    "    data4d[:,0,:,:] = data\n",
    "    return data4d\n",
    "\n",
    "def val_tarl4d(data):\n",
    "    image_size = [1,10]\n",
    "    data4d = np.zeros([Total_Val_Photo,1,image_size[0],image_size[1]])\n",
    "    data4d[:,0,:,:] = data\n",
    "    return data4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1, 45, 35)\n",
      "(3750, 1, 45, 35)\n",
      "(1800, 1, 1, 10)\n",
      "(3750, 1, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "input_train4d = train_in4d(input_data_raw)\n",
    "input_val4d = val_inl4d(val_data_raw)\n",
    "\n",
    "target_train4d = train_tar4d(target_train3d)\n",
    "target_val4d = val_tarl4d(target_val3d)\n",
    "\n",
    "print(input_train4d.shape)\n",
    "print(input_val4d.shape)\n",
    "print(target_train4d.shape)\n",
    "print(target_val4d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "epoch_max = 10000\n",
    "learning_rate = 0.2\n",
    "batch_size_train = int(Total_Train_Photo/10)\n",
    "batch_size_val = int(Total_Val_Photo/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1800 images in the training set\n",
      "There are 3750 images in the test set\n",
      "There are 10 batches in the train loader\n",
      "There are 10 batches in the testloader\n"
     ]
    }
   ],
   "source": [
    "input_train4d = torch.from_numpy(input_train4d).float().cuda()\n",
    "target_train4d = torch.from_numpy(target_train4d).float().cuda()\n",
    "input_val4d = torch.from_numpy(input_val4d).float().cuda()\n",
    "target_val4d = torch.from_numpy(target_val4d).float().cuda()\n",
    "\n",
    "target_train = torch.from_numpy(target_train).float().cuda()\n",
    "target_val = torch.from_numpy(target_val).float().cuda()\n",
    "\n",
    "dataset_train = torch.utils.data.TensorDataset(input_train4d, target_train)\n",
    "dataset_val = torch.utils.data.TensorDataset(input_val4d, target_val)\n",
    "\n",
    "kwargs = {}\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train,\n",
    "                                           batch_size=batch_size_train, \n",
    "                                           shuffle=True,**kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val,\n",
    "                                         batch_size=batch_size_val, \n",
    "                                         shuffle=True,**kwargs)\n",
    "\n",
    "print('There are {} images in the training set'.format(len(dataset_train)))\n",
    "print('There are {} images in the test set'.format(len(dataset_val)))\n",
    "print('There are {} batches in the train loader'.format(len(train_loader)))\n",
    "print('There are {} batches in the testloader'.format(len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5, stride=1, padding=0),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(10, 30, kernel_size=3, stride=1, padding=0))\n",
    "        self.fc1 = nn.Linear(38 * 28 * 30, 500)\n",
    "        self.act1 = nn.Tanh()\n",
    "    \n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.act2 = nn.Tanh()\n",
    "        \n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        self.act3 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(-1, 38 * 28 * 30)\n",
    "        out = self.act1(self.fc1(out))\n",
    "        out = self.act2(self.fc2(out))\n",
    "        out = self.act3(self.fc3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=4, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(20, 40, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=28160, out_features=1000, bias=True)\n",
      "  (act1): Tanh()\n",
      "  (fc2): Linear(in_features=1000, out_features=500, bias=True)\n",
      "  (act2): Tanh()\n",
      "  (fc3): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (act3): Sigmoid()\n",
      ")\n",
      "torch.Size([20, 1, 5, 5])\n",
      "torch.Size([20])\n",
      "torch.Size([40, 20, 5, 5])\n",
      "torch.Size([40])\n",
      "torch.Size([1000, 28160])\n",
      "torch.Size([1000])\n",
      "torch.Size([500, 1000])\n",
      "torch.Size([500])\n",
      "torch.Size([10, 500])\n",
      "torch.Size([10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "model = model.cuda()    \n",
    "loss_fn = torch.nn.MSELoss(size_average='false').cuda()        \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.3)\n",
    "print(model)\n",
    "for p in model.parameters():\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12000, Training Loss: 1.2728960, Training Acc: 0.056\n",
      "Epoch 2/12000, Training Loss: 0.9004024, Training Acc: 0.000\n",
      "Epoch 3/12000, Training Loss: 0.8975847, Training Acc: 0.000\n",
      "Epoch 4/12000, Training Loss: 0.8952391, Training Acc: 0.000\n",
      "Epoch 5/12000, Training Loss: 0.8924547, Training Acc: 0.000\n",
      "Epoch 6/12000, Training Loss: 0.8898296, Training Acc: 0.000\n",
      "Epoch 7/12000, Training Loss: 0.8868768, Training Acc: 0.000\n",
      "Epoch 8/12000, Training Loss: 0.8831644, Training Acc: 0.000\n",
      "Epoch 9/12000, Training Loss: 0.8788415, Training Acc: 0.000\n",
      "Epoch 10/12000, Training Loss: 0.8741702, Training Acc: 0.000\n",
      "Epoch 11/12000, Training Loss: 0.8691750, Training Acc: 0.000\n",
      "Epoch 12/12000, Training Loss: 0.8633217, Training Acc: 0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0b82a02712af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mlabels_recog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhitung\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs_recog\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhitung\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_recog\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhitung\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                 \u001b[0mbenar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbenar\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training dan Validasi CNN\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "#Train\n",
    "\n",
    "for epoch in range(epoch_max):\n",
    "    benar = 0\n",
    "    salah = 0\n",
    "    iter_loss = 0.0\n",
    "    iterations = 0\n",
    "    model.train()\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        iter_loss = iter_loss+loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        outputs_recog = torch.round(outputs)\n",
    "        labels_recog = torch.round(labels)\n",
    "        for hitung in range(batch_size_train):\n",
    "            if torch.equal(outputs_recog[hitung,:], labels_recog[hitung,:]):\n",
    "                benar = benar + 1\n",
    "            else:\n",
    "                salah = salah + 1\n",
    "        iterations = iterations + 1\n",
    "                    \n",
    "    recognition_rate = (benar/Total_Train_Photo)*100\n",
    "    train_accuracy.append(recognition_rate)\n",
    "    train_loss.append(iter_loss)\n",
    "    \n",
    "    if (epoch+1)%1000 == 0:\n",
    "        torch.save(model.state_dict(),'CNN_3DFaceRecog_Save_3070_Col_v8.pth')\n",
    "    \n",
    "#Validasi \n",
    "    if (epoch+1)%100 == 0:\n",
    "        loss_iter_val = 0.0\n",
    "        benar_val = 0\n",
    "        salah_val = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss_v = loss_fn(outputs, labels)\n",
    "            loss_iter_val = loss_iter_val+loss_v.item()\n",
    "            outputs_recog = torch.round(outputs)\n",
    "            labels_recog = torch.round(labels)\n",
    "            for hitung in range(batch_size_val):\n",
    "                if torch.equal(outputs_recog[hitung,:], labels_recog[hitung,:]):\n",
    "                    benar_val = benar_val + 1\n",
    "                else:\n",
    "                    salah_val = salah_val + 1\n",
    "            iterations = iterations + 1\n",
    "\n",
    "        recognition_rate_v = (benar_val/Total_Val_Photo)*100\n",
    "        val_accuracy.append(recognition_rate_v)\n",
    "        val_loss.append(loss_iter_val)\n",
    "    \n",
    "    print ('Epoch {}/{}, Training Loss: {:.7f}, Training Acc: {:.3f}'.\n",
    "           format(epoch+1, epoch_max, \n",
    "                  train_loss[-1], \n",
    "                  train_accuracy[-1])); \n",
    "    if (epoch+1)%100 == 0:\n",
    "        print('\\nValidation || Validation Loss: {:.7f}, Validation Acc: {:.3f}\\n'\n",
    "              .format(val_loss[-1],\n",
    "                     val_accuracy[-1]))\n",
    "end = time.time()\n",
    "\n",
    "elapsed = end - start\n",
    "print('Elapased Time: ', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHMVJREFUeJzt3Xm0HHWd9/H3J8sNCWSDe8MeEpbpK/jgoAFB5iiKPg+MC6LOCIOCjj4cBQRGZ9yO48LMPOijzggjo6KDgCCeOYrI+DggIshRliGIRJAtIksSIAtbWEJI8n3+qGpvdaf73r7crlvdVZ/XOX26uqq6+9sR+3N/9av+liICMzOzuilFF2BmZr3FwWBmZg0cDGZm1sDBYGZmDRwMZmbWwMFgZmYNHAzW9yQdLekhSU9LOkDSHZIOS7d9VtJFBZeYG0nXSnp/unycpJ92su8E3u+Tkr41kdew3udgsEkl6RhJN0l6RtLqdPkkSZrAy34JOCUitouIWyNiv4i49kXUdr6kkPSWpvVfSde/J308IOnLklakYfQHSf+S2f9+Sc+l2+q3r7Z4v2PTfdW0flr6b/Om8dQfERdHxP8c14cep4j4PxExoXCx3udgsEkj6SPAWcAXgZ2AHYEPAIcCA22eM7WDl94DuKNLZd4DnJB5/2nAXwC/z+zzCWAJcBAwG3gtcGvT67w5Dar67ZQW7/VDYB7wmqb1RwABXDGRD2L2YjkYbFJImgucAZwUEd+PiPWRuDUijouI59P9zpf0NUk/kfQM8FpJb5R0q6Sn0kNGn033nSHpaWAqcJuk36fr75f0+hY1TJd0iaQfSGoZRMB/AodKmp8+PgJYBjyS2edA4IcRsSr9DPdHxIXj/TeJiA3AfwDHN206Hrg4IjZJmi/px5LWSHo8Xd6t1etJeo+kX2Yev0HSXZKeTEcsymzbS9LPJa2TtFbSxZLmZbZ/TNJKSesl3S3p8HR9qQ/NWcLBYJPlEGAG8KMO9v0r4J9I/hr/JfAMyZflPOCNwAclvTUino+I7dLnvCwi9mr3gpJmApcBzwN/GREb2+y6AbgcOCZ9fDzQ/KV/I/Dh9BDY/5jgYbALgHek9dUD9M2Z95wCfJtkVLQQeA7Y6rBUM0mDwA+ATwGDJCOeQ7O7AGcCuwAvAXYHPps+twacAhwYEbOB/wXc/+I/ovUbB4NNlkFgbURsqq+QdL2kJ9Lj8a/O7PujiPhVRGyJiA0RcW1E/DZ9vAy4hK0Pv4xmDslhmd8D742IzWPsfyFwfPol/RqSQMk6E/gCcBywFFgp6YSmfS5LP1v99r9bvVFE/Ap4FDg6XfWXwD0R8Zt0+7qI+EFEPBsR60kCs5PP/ufA79LR2QvAV8iMeiJieURclYbrGuCfM6+7mSTE95U0PR0R/b75Day8HAw2WdYBg+kxewAi4lURMS/dlv1v8aHsEyW9UtI16eGUJ0nmJQbH8d4HA/sDn48OukZGxC+BIZK/tn8cEc81bd8cEedExKEko5h/As6T9JLMbm+NiHmZ2zdHecsLGTmc9G6SUQQAkmZJ+oakByQ9BVwHzOtg7mUXMv+O6ef+42NJCyR9Lz1c9BRwEem/aUQsB04nGUGsTvfbZYz3sxJxMNhkuYHkMM5RHezb/OX9XZLDO7tHxFzg62SOl3fgpyR/5V8taccOn3MR8BG2PozUWGjEcxFxDvA4sO84asq6EDhc0iEkIfbdzLaPADXglRExB6iPrMb6/A+THB5Kdk4Od+2e2X4myb/z/unrviv7mhHx3Yj4M5JDWEEyQrKKcDDYpIiIJ4DPAf8m6R2StpM0RdKfAtuO8fTZwGMRsUHSQSRzEON9//9L8oV7dXr8fSxnA28g+Qu9gaTTJR0maWZ6aukJaY3NZyZ1WtsDJHMplwBXRUR2ons2ybzCE5K2Bz7T4cv+P2A/SW9LR2mnkpwJln3dp9PX3RX4u8znq0l6naQZJHMuz5EcXrKKcDDYpEm/nD8MfBRYTXJs/RvAx4DrR3nqScAZktYDnyY5k+fFvP8/kMwX/Cz9kh1t38ci4uo2h56eA75Mcsx+LXAy8PaIuC+zz382/Y7hh2OUdwHJX+fNI5SvADPT97mRDk9hjYi1JKfZfp7kUN0+wK8yu3wOeDnwJEmIXJrZNiN93tr0My4APtnJ+1o5yBfqMTOzLI8YzMysgYPBzMwaOBjMzKyBg8HMzBpMG3uX3jM4OBiLFi0qugwzs75yyy23rI2IobH268tgWLRoEUuXLi26DDOzviLpgU7286EkMzNr4GAwM7MGDgYzM2vgYDAzswYOBjMza+BgMDOzBg4GMzNrUKlguPFG+OhHYcuWoisxM+tdlQqGZcvgi1+EBx8suhIzs95VqWCo1ZL7u+8utg4zs15WqWAYHk7uHQxmZu1VKhgWLIC5c+Guu4quxMysd1UqGKRk1OARg5lZe5UKBkjmGTxiMDNrr5LBsGoVrF9fdCVmZr2pcsFQn4C+555i6zAz61WVC4b6Kas+nGRm1lrlgmHvvWHKFE9Am5m1U7lgmDEDFi/2iMHMrJ3KBQMkh5M8YjAza62SwTA8nEw+u5memdnWKhkMtRps2OBmemZmrVQyGNwzycysvUoGg09ZNTNrr5LBUG+m5xGDmdnWKhkMbqZnZtZeJYMB3EzPzKydygbD8LCb6ZmZtVLZYPBlPs3MWnMwOBjMzBpUNhjcTM/MrLXKBoOb6ZmZtVbZYACfsmpm1kqlg6FWczM9M7NmlQ8GN9MzM2tU6WBwMz0zs61VOhjcTM/MbGuVDoYFC2DePI8YzMyyKh0MknsmmZk1q3QwgE9ZNTNrlmswSDpP0mpJt7fZfpykZentekkvy7OeVmo1N9MzM8vKe8RwPnDEKNv/ALwmIvYH/gE4N+d6tuKeSWZmjXINhoi4DnhslO3XR8Tj6cMbgd3yrKcVn7JqZtaol+YY3gf8V7uNkk6UtFTS0jVr1nTtTffaK2mm5wloM7NETwSDpNeSBMPH2u0TEedGxJKIWDI0NNS1954xA/bc0yMGM7O6aUUXIGl/4FvAkRGxrogaajUHg5lZXaEjBkkLgUuBd0fEPUXV4WZ6ZmYjch0xSLoEOAwYlLQC+AwwHSAivg58GtgB+DdJAJsiYkmeNbUyPDzSTG/Rosl+dzOz3pJrMETEsWNsfz/w/jxr6ES2Z5KDwcyqricmn4vmU1bNzEY4GIChITfTMzOrczDgZnpmZlkOhpSb6ZmZJRwMqXozvaeeKroSM7NiORhS9Qnoewr7NYWZWW9wMKTcZdXMLOFgSLmZnplZwsGQcjM9M7OEgyHDzfTMzBwMDYaH3UzPzMzBkFGrjTTTMzOrKgdDRraZnplZVTkYMtxMz8zMwdDAzfTMzBwMDaRk1OBDSWZWZQ6GJj5l1cyqzsHQxM30zKzqHAxN3EzPzKrOwdDEzfTMrOocDE322gumTvUEtJlVl4OhyYwZsHixRwxmVl0OhhZ8/WczqzIHQwvDw3DvvW6mZ2bV5GBowc30zKzKHAwt1E9Z9eEkM6siB0MLPmXVzKrMwdBCvZmeRwxmVkUOhhbqzfQ8YjCzKnIwtOFmemZWVQ6GNoaH3UzPzKrJwdBGfQLazfTMrGocDG34+s9mVlUOhjbqzfQ8z2BmVeNgaMPN9MysqjoOBkl/IulqSbenj/eX9Kn8Siuer/9sZlU0nhHDN4FPAC8ARMQy4Jg8iuoVtVrSTG/z5qIrMTObPOMJhlkR8d9N6zZ1s5he42Z6ZlZF4wmGtZL2AgJA0juAh3OpqkfUm+l5nsHMqmQ8wXAy8A1gWNJK4HTgg7lU1SPcTM/MqqjjYIiI+yLi9cAQMBwRfxYR94/2HEnnSVpdn7BusX1Y0g2Snpf0t+OqfBIMDcH8+Z6ANrNqmdbpjpI+3fQYgIg4Y5SnnQ98FbiwzfbHgFOBt3Zax2SS3DPJzKpnPIeSnsncNgNHAotGe0JEXEfy5d9u++qIuJn0TKde5FNWzaxqOh4xRMSXs48lfQm4vOsVtSHpROBEgIULF07W21KrwfnnJ8305syZtLc1MyvMRH75PAvYs1uFjCUizo2IJRGxZGhoaLLe1s30zKxyxjPH8FvSU1WBqSST0KPNL5RC9vrPS5YUW4uZ2WToOBiAN2WWNwGPRkSpf+AGbqZnZtUzZjBI2j5dXN+0aY4kIqLt5LKkS4DDgEFJK4DPANMBIuLrknYClgJzgC2STgf2jYieuTzOwADsuacnoM2sOjoZMdxCcghJLbYFo8wzRMSxo71wRDwC7NZBDYXyKatmViVjBkNELJ6MQnpZrQY/+1nSTG/q1KKrMTPL13jmGJA0H9gH2Ka+Lv2tQqkND48001tc+Zg0s7Ibz1lJ7wdOIzn08xvgYOAG4HX5lNY7sj2THAxmVnbj+R3DacCBwAMR8VrgAGBNLlX1mOwpq2ZmZTeeYNgQERsAJM2IiLuAWj5l9ZbBwaSZniegzawKxjPHsELSPOAy4CpJjwOr8imrt7iZnplVyXh6JR2dLn5W0jXAXOCKXKrqQcPDcOWVRVdhZpa/jg8lSTpL0qsAIuIXEXF5RGzMr7TeUqvBww8nzfTMzMpsPHMMvwY+JWm5pC9KqlTnIF/m08yqYjxXcLsgIv4cOAi4B/iCpHtzq6zH+DKfZlYVL6bt9t7AMMlFeipzAqeb6ZlZVYxnjqE+QjgDuB14RUS8ObfKeoyb6ZlZVYzndNU/AIdExNpWGyXtFxF3dKes3uRTVs2sCsYzx/D1dqGQ+k4X6ulpw8PJldw2by66EjOz/Ezk0p7NWrXlLpVaDZ5/PmmmZ2ZWVt0Mhhh7l/7mM5PMrAq6GQyl52Z6ZlYF3QyG0v8K2s30zKwKxgwGSe/KLB/atO2U+nJEHNzd0nqPlIwaHAxmVmadjBg+nFn+16Ztf93FWvpCreZDSWZWbp0Eg9ost3pcem6mZ2Zl10kwRJvlVo9Lz830zKzsOvnl87CkZSSjg73SZdLHe+ZWWY/KnrJ64IHF1mJmlodOguEluVfRR9xMz8zKbsxgiIgHso8l7QC8GngwIm7Jq7Be5WZ6ZlZ2nZyu+mNJL02XdybprPrXwHcknZ5zfT3JzfTMrMw6mXxeHBG3p8vvBa5K222/kgqergpupmdm5dZJMLyQWT4c+AlARKwHtuRRVK9zMz0zK7NOguEhSR+SdDTwcuAKAEkzgel5FterfMqqmZVZJ8HwPmA/4D3AOyPiiXT9wcC3c6qrp9VPWfUEtJmVUSdnJa0GPtBi/TXANXkU1evcTM/MymzMYJB0+WjbI+It3SunP9Sb6XnEYGZl1MkP3A4BHgIuAW6igv2RWqnV4Mori67CzKz7Oplj2An4JPBS4CzgDcDaiPhFRPwiz+J62fCwm+mZWTmNGQwRsTkiroiIE0gmnJcD10r6UO7V9TBf5tPMyqqjK7hJmiHpbcBFwMnA2cCleRbW6xwMZlZWnUw+X0ByGOm/gM9lfgVdafVmep6ANrOy6WTy+d3AM8CfAKdKf5x7FhARMSen2npavZmeRwxmVjad/I6ho8NNVeTrP5tZGeX6pS/pPEmrJbU8/KTE2ZKWS1om6eV51tNttZqb6ZlZ+eQ9GjgfOGKU7UcC+6S3E4Gv5VxPV7mZnpmVUa7BEBHXAY+NsstRwIWRuBGYl17zoS/Um+l5AtrMyqTo+YNdSX5VXbciXbcVSSdKWipp6Zo1ayaluLH4lFUzK6Oig6FVe41otWNEnBsRSyJiydDQUM5ldWZwELbf3sFgZuVSdDCsAHbPPN4NWFVQLeMmJaMGH0oyszIpOhguB45Pz046GHgyIh4uuKZx8fWfzaxs8j5d9RLgBqAmaYWk90n6gKT69R1+AtxH0n/pm8BJedaTBzfTM7Oy6eSXzy9aRBw7xvYg6b3Ut7IT0AceWGwtZmbdUPShpL7n6z+bWdk4GCZozz3dTM/MysXBMEEDA0mnVY8YzKwsHAxd4FNWzaxMHAxdUKvBvfe6mZ6ZlYODoQuGh91Mz8zKw8HQBfVTVn04yczKwMHQBT5l1czKxMHQBfVmeh4xmFkZOBi6xD2TzKwsHAxd4us/m1lZOBi6pFZzMz0zKwcHQ5d4AtrMysLB0CU+ZdXMysLB0CX1ZnoeMZhZv3MwdImb6ZlZWTgYusjN9MysDBwMXTQ87GZ6Ztb/HAxdVKslzfQeeKDoSszMXjwHQxdlr/9sZtavHAxd5N8ymFkZOBi6yM30zKwMHAxd5p5JZtbvHAxd5lNWzazfORi6rFaDRx5xMz0z618Ohi7zBLSZ9TsHQ5e5mZ6Z9TsHQ5fttRdMm+YRg5n1LwdDl02fnnRadTCYWb9yMOTAZyaZWT9zMOTAzfTMrJ85GHLgZnpm1s8cDDnwKatm1s8cDDlwl1Uz62cOhhy4mZ6Z9TMHQ07cTM/M+pWDISc+ZdXM+pWDISfDw26mZ2b9ycGQE09Am1m/yj0YJB0h6W5JyyV9vMX2PSRdLWmZpGsl7ZZ3TZPBzfTMrF/lGgySpgLnAEcC+wLHStq3abcvARdGxP7AGcCZedY0WdxMz8z61bScX/8gYHlE3Acg6XvAUcDvMvvsC/xNunwNcFnONU2KejO9Cy9M2mPMng3bbdf6vt22gYGiP4WZVVHewbAr8FDm8QrglU373Aa8HTgLOBqYLWmHiFiX3UnSicCJAAsXLsyt4G466ST4znfgtttg/Xp4+unkPqKz5w8MjB4orYJl221h1qz297NmJSMZM7N28v6KUIt1zV+Lfwt8VdJ7gOuAlcCmrZ4UcS5wLsCSJUs6/Got1mmnJbesCHjuuSQgsmFRv2+1Lrtt/XpYtapx28aN46trYKAxKMYKk9Hum0Nq5kxQq//Vzaxv5B0MK4DdM493A1Zld4iIVcDbACRtB7w9Ip7Mua7CSCNfyDvu2J3X3LhxJDSefTa5PfPM+O/Xr09OsW1eP54usVOmJCFRv7Ub6Yxn2/Tp3fl3MrPO5B0MNwP7SFpMMhI4Bvir7A6SBoHHImIL8AngvJxrKp2BAdhhh+SWh40bW4fJ00+P3Oqjl3ajnZUrt95vPJ8vGxr1kcpEHm+7bRJiZra1XIMhIjZJOgW4EpgKnBcRd0g6A1gaEZcDhwFnSgqSQ0kn51mTjd/AQHKbN697r7llSxIuYwVK9jDaM88kj+v369aNLNfXdzp/A42Hw1qFSH3+JnubM6f1+tmzfbKAlYdiPP9P6hFLliyJpUuXFl2G9Zj6/E02KLKjmuYQGW1bNpQ2bOjs/WfMGDtE2gXLnDkwf34SvnPnejRj+ZB0S0QsGWs/n59ipZGdv1mwoHuv+8ILjZP/69cnrU46WbdmDdx3X+OJBWP9LSY1BsX8+Y3L7e7ry9ts073PbtXkYDAbw/TpSRv17bef+Gtt2TIyIsnennwSnngiuT3++Mh9ffnuu0fWPfvs6O+xzTZjh8j224/MSw0OJvfz58PUqRP/jNb/HAxmk2jKlJHDRy/Wxo2NIZINklb3jz6atGapP2fLltavKyXh0BwYoy3vsIPnVsrIwWDWZwYGkkNlL+Zw2ZYtyQjl8cdh7dpkAn/dusbl+uOVK2HZsmR5tFHK7NljB8ngIAwNJbfBQZ+C3OscDGYVMmVKMrk9dy4sWtT58zZs2DpAWoXJunWwfHmy/OQov0aaO3ckKLK3bIBkb7NmTfij2zg4GMxsTNtsA7vumtw6tWkTPPZYEhJr1yYT8a1u998PN9+c7PPCC61fa9as9qHRav3cuf4F/kQ4GMwsF9Omje+QV0RyZle7AFmzZiRg7rwzuW93iCt7uG3BgqTLQLvloSEf2mrmYDCzniCNHObae+/OnvPss60DZPXqkdujj8IddyT37fqKzZ8/eoBkH8+ZU/7RiIPBzPrWrFmwxx7JbSz1EUk2MJoDZPVquP32ZPnxx1u/zowZjYGx006wyy7JbdddR5Z33LF/Oxn3adlmZuOTHZHss8/Y+2/cmBy6ahcg9eXbbkuaTzafBiwl4dAqNLK3wcHe+6W7g8HMrIWBgZEv77Fs3pwExapVyW3lypHlVavgoYfgppuSw1zNpk+HnXduHRrZQJnMCXUHg5nZBE2dmny577wzvOIV7ffbuDEZXbQKj1Wrkh8i/vznyQ8Rm82cmQTE3/89nHBCfp8FHAxmZpNmYAAWLkxuo3n22a1Do34bGsq/TgeDmVmPmTUrOTOr07Ozuq3HpjzMzKxoDgYzM2vgYDAzswYOBjMza+BgMDOzBg4GMzNr4GAwM7MGDgYzM2ugiCi6hnGTtAZ44EU+fRBY28Vyek2ZP58/W/8q8+frp8+2R0SM+dvpvgyGiZC0NCKWFF1HXsr8+fzZ+leZP18ZP5sPJZmZWQMHg5mZNahiMJxbdAE5K/Pn82frX2X+fKX7bJWbYzAzs9FVccRgZmajcDCYmVmDSgWDpCMk3S1puaSPF11Pt0jaXdI1ku6UdIek04quqdskTZV0q6QfF11Lt0maJ+n7ku5K/zc8pOiaukXS36T/Td4u6RJJ2xRd00RIOk/Sakm3Z9ZtL+kqSfem9/OLrLEbKhMMkqYC5wBHAvsCx0rat9iqumYT8JGIeAlwMHByiT5b3WnAnUUXkZOzgCsiYhh4GSX5nJJ2BU4FlkTES4GpwDHFVjVh5wNHNK37OHB1ROwDXJ0+7muVCQbgIGB5RNwXERuB7wFHFVxTV0TEwxHx63R5PckXy67FVtU9knYD3gh8q+hauk3SHODVwL8DRMTGiGhxKfi+NQ2YKWkaMAtYVXA9ExIR1wGPNa0+CrggXb4AeOukFpWDKgXDrsBDmccrKNGXZ52kRcABwE3FVtJVXwE+CmwpupAc7AmsAb6dHir7lqRtiy6qGyJiJfAl4EHgYeDJiPhpsVXlYseIeBiSP9KABQXXM2FVCga1WFeqc3UlbQf8ADg9Ip4qup5ukPQmYHVE3FJ0LTmZBrwc+FpEHAA8QwkORQCkx9qPAhYDuwDbSnpXsVVZJ6oUDCuA3TOPd6PPh7VZkqaThMLFEXFp0fV00aHAWyTdT3L473WSLiq2pK5aAayIiPoI7/skQVEGrwf+EBFrIuIF4FLgVQXXlIdHJe0MkN6vLrieCatSMNwM7CNpsaQBkkmwywuuqSskieQY9Z0R8c9F19NNEfGJiNgtIhaR/G/284gozV+dEfEI8JCkWrrqcOB3BZbUTQ8CB0ualf43ejglmVhvcjlwQrp8AvCjAmvpimlFFzBZImKTpFOAK0nOjjgvIu4ouKxuORR4N/BbSb9J130yIn5SYE3WuQ8BF6d/sNwHvLfgeroiIm6S9H3g1yRnzt1Kn7ePkHQJcBgwKGkF8Bng88B/SHofSRj+RXEVdodbYpiZWYMqHUoyM7MOOBjMzKyBg8HMzBo4GMzMrIGDwczMGjgYzDIkbZb0m8yta79ClrQo25XTrFdV5ncMZh16LiL+tOgizIrkEYNZByTdL+kLkv47ve2drt9D0tWSlqX3C9P1O0r6oaTb0lu9FcRUSd9Mr1HwU0kz0/1PlfS79HW+V9DHNAMcDGbNZjYdSnpnZttTEXEQ8FWSjq+kyxdGxP7AxcDZ6fqzgV9ExMtIeh/Vf2W/D3BOROwHPAG8PV3/ceCA9HU+kNeHM+uEf/lsliHp6YjYrsX6+4HXRcR9acPCRyJiB0lrgZ0j4oV0/cMRMShpDbBbRDyfeY1FwFXpBV2Q9DFgekT8o6QrgKeBy4DLIuLpnD+qWVseMZh1Ltost9unleczy5sZmed7I8kVBl8B3JJe2MasEA4Gs869M3N/Q7p8PSOXqzwO+GW6fDXwQfjj9arntHtRSVOA3SPiGpILEs0Dthq1mE0W/1Vi1mhmpkMtJNdirp+yOkPSTSR/UB2brjsVOE/S35Fcia3eGfU04Ny04+ZmkpB4uM17TgUukjSX5IJS/1Kyy3tan/Ecg1kH0jmGJRGxtuhazPLmQ0lmZtbAIwYzM2vgEYOZmTVwMJiZWQMHg5mZNXAwmJlZAweDmZk1+P92wPDJyz0cNAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch validation loss: 0.8633217066526413\n"
     ]
    }
   ],
   "source": [
    "plt.plot(train_loss, 'b') # 'r' is the color red\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss_value')\n",
    "plt.title('Loss Graph Training')\n",
    "plt.show()\n",
    "print('Last epoch training loss: {}' .format(train_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'CNN_3DFaceRecog_Save_3070_Col_v8.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_3DFaceRecognition_v7_3070_Col_TV\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f26a1147340a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                    \u001b[0mtrain_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                    \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                    \u001b[0mval_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m                    val_accuracy[-1]))\n\u001b[0;32m      8\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('CNN_3DFaceRecognition_v8_3070_Col_TV')\n",
    "print('Epochs: {}, Training Loss: {:.7f}, Training Acc: {:.3f}, Validation Loss: {:.7f}, Validation Acc: {:.3f}'\n",
    "           .format(epoch+1,\n",
    "                   train_loss[-1],\n",
    "                   train_accuracy[-1],\n",
    "                   val_loss[-1],\n",
    "                   val_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.savemat('Epochs_and_time_3070C_v8.mat', {'Epochs':epoch, 'time':elapsed, 'train_loss':train_loss, 'train_accuracy':train_accuracy})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
