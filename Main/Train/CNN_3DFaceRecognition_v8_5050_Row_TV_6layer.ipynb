{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "#%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('Train_Raw.mat')\n",
    "Train_Raw = mat['Train_Raw']\n",
    "mat1 = scipy.io.loadmat('Val_Raw.mat')\n",
    "Val_Raw = mat1['Val_Raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2960\n",
      "2590\n"
     ]
    }
   ],
   "source": [
    "l,w = 45,35\n",
    "Total_Train_Photo = int((Train_Raw.shape[0]*Train_Raw.shape[1])/(l*w))\n",
    "Total_Val_Photo = int((Val_Raw.shape[0]*Val_Raw.shape[1])/(l*w))\n",
    "Photo_Train_Col = 37\n",
    "Photo_Val_Col = 37\n",
    "print(Total_Train_Photo)\n",
    "print(Total_Val_Photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalisasi\n",
    "Train_Raw_Norm = Train_Raw/255\n",
    "Val_Raw_Norm = Val_Raw/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2960, 45, 35)\n",
      "(2590, 45, 35)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "input_data_raw = np.zeros([Total_Train_Photo,l,w])\n",
    "val_data_raw = np.zeros([Total_Val_Photo,l,w])\n",
    "for a in range(0,int(Total_Train_Photo/Photo_Train_Col)):\n",
    "    for b in range(0,Photo_Train_Col):\n",
    "        input_data_raw[i,:,:] = Train_Raw_Norm[45*a:45+45*a,35*b:35+35*b]\n",
    "        i=i+1\n",
    "i,a,b=0,0,0\n",
    "for a in range(0,int(Total_Val_Photo/Photo_Val_Col)):\n",
    "    for b in range(0,Photo_Val_Col):\n",
    "        val_data_raw[i,:,:] = Val_Raw_Norm[45*a:45+45*a,35*b:35+35*b]\n",
    "        i=i+1\n",
    "        \n",
    "print(input_data_raw.shape)\n",
    "print(val_data_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(input_data_raw[2959,:,:], cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2960, 10)\n",
      "(2590, 10)\n"
     ]
    }
   ],
   "source": [
    "tar = np.eye(10)\n",
    "target_train = np.zeros([Total_Train_Photo,10])\n",
    "target_val = np.zeros([Total_Val_Photo,10])\n",
    "for d in range(0,10):    \n",
    "    target_train[0+int(Total_Train_Photo/10)*d:int(Total_Train_Photo/10)+int(Total_Train_Photo/10)*d,:] = tar[d,:]\n",
    "d=0\n",
    "for d in range(0,10):\n",
    "    target_val[0+int(Total_Val_Photo/10)*d:int(Total_Val_Photo/10)+int(Total_Val_Photo/10)*d,:] = tar[d,:]\n",
    "print(target_train.shape)\n",
    "print(target_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tebal_train, tebal_val = Total_Train_Photo, Total_Val_Photo\n",
    "target_train3d = np.zeros(shape=(tebal_train, 1, 10))\n",
    "target_val3d = np.zeros(shape=(tebal_val, 1, 10))\n",
    "i=0\n",
    "\n",
    "for i in range(tebal_train):\n",
    "    target_train3d[i,0,:] = target_train[i,:]\n",
    "for i in range(tebal_val):\n",
    "    target_val3d[i,0,:] = target_val[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_in4d(data):\n",
    "    image_size = [45,35]\n",
    "    data4d = np.zeros([Total_Train_Photo,1,image_size[0],image_size[1]])\n",
    "    data4d[:,0,:,:] = data\n",
    "    return data4d\n",
    "\n",
    "def val_inl4d(data):\n",
    "    image_size = [45,35]\n",
    "    data4d = np.zeros([Total_Val_Photo,1,image_size[0],image_size[1]])\n",
    "    data4d[:,0,:,:] = data\n",
    "    return data4d\n",
    "\n",
    "def train_tar4d(data):\n",
    "    image_size = [1,10]\n",
    "    data4d = np.zeros([Total_Train_Photo,1,image_size[0],image_size[1]])\n",
    "    data4d[:,0,:,:] = data\n",
    "    return data4d\n",
    "\n",
    "def val_tarl4d(data):\n",
    "    image_size = [1,10]\n",
    "    data4d = np.zeros([Total_Val_Photo,1,image_size[0],image_size[1]])\n",
    "    data4d[:,0,:,:] = data\n",
    "    return data4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2960, 1, 45, 35)\n",
      "(2590, 1, 45, 35)\n",
      "(2960, 1, 1, 10)\n",
      "(2590, 1, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "input_train4d = train_in4d(input_data_raw)\n",
    "input_val4d = val_inl4d(val_data_raw)\n",
    "\n",
    "target_train4d = train_tar4d(target_train3d)\n",
    "target_val4d = val_tarl4d(target_val3d)\n",
    "\n",
    "print(input_train4d.shape)\n",
    "print(input_val4d.shape)\n",
    "print(target_train4d.shape)\n",
    "print(target_val4d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "epoch_max = 10000\n",
    "learning_rate = 0.2\n",
    "batch_size_train = int(Total_Train_Photo/10)\n",
    "batch_size_val = int(Total_Val_Photo/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2960 images in the training set\n",
      "There are 2590 images in the test set\n",
      "There are 10 batches in the train loader\n",
      "There are 10 batches in the testloader\n"
     ]
    }
   ],
   "source": [
    "input_train4d = torch.from_numpy(input_train4d).float().cuda()\n",
    "target_train4d = torch.from_numpy(target_train4d).float().cuda()\n",
    "input_val4d = torch.from_numpy(input_val4d).float().cuda()\n",
    "target_val4d = torch.from_numpy(target_val4d).float().cuda()\n",
    "\n",
    "target_train = torch.from_numpy(target_train).float().cuda()\n",
    "target_val = torch.from_numpy(target_val).float().cuda()\n",
    "\n",
    "dataset_train = torch.utils.data.TensorDataset(input_train4d, target_train)\n",
    "dataset_val = torch.utils.data.TensorDataset(input_val4d, target_val)\n",
    "\n",
    "kwargs = {}\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train,\n",
    "                                           batch_size=batch_size_train, \n",
    "                                           shuffle=True,**kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val,\n",
    "                                         batch_size=batch_size_val, \n",
    "                                         shuffle=True,**kwargs)\n",
    "\n",
    "print('There are {} images in the training set'.format(len(dataset_train)))\n",
    "print('There are {} images in the test set'.format(len(dataset_val)))\n",
    "print('There are {} batches in the train loader'.format(len(train_loader)))\n",
    "print('There are {} batches in the testloader'.format(len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5, stride=1, padding=0),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(10, 30, kernel_size=3, stride=1, padding=0))\n",
    "        self.fc1 = nn.Linear(38 * 28 * 30, 500)\n",
    "        self.act1 = nn.Tanh()\n",
    "    \n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.act2 = nn.Tanh()\n",
    "        \n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        self.act3 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(-1, 38 * 28 * 30)\n",
    "        out = self.act1(self.fc1(out))\n",
    "        out = self.act2(self.fc2(out))\n",
    "        out = self.act3(self.fc3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(10, 30, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (fc1): Linear(in_features=31920, out_features=500, bias=True)\n",
      "  (act1): Tanh()\n",
      "  (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (act2): Tanh()\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (act3): Sigmoid()\n",
      ")\n",
      "torch.Size([10, 1, 5, 5])\n",
      "torch.Size([10])\n",
      "torch.Size([30, 10, 3, 3])\n",
      "torch.Size([30])\n",
      "torch.Size([500, 31920])\n",
      "torch.Size([500])\n",
      "torch.Size([100, 500])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "model = model.cuda()    \n",
    "loss_fn = torch.nn.MSELoss(size_average='false').cuda()        \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.3)\n",
    "print(model)\n",
    "for p in model.parameters():\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Training Loss: 0.8941735, Training Acc: 0.000\n",
      "Epoch 2/25, Training Loss: 0.8933569, Training Acc: 0.000\n",
      "Epoch 3/25, Training Loss: 0.8924093, Training Acc: 0.000\n",
      "Epoch 4/25, Training Loss: 0.8913352, Training Acc: 0.000\n",
      "Epoch 5/25, Training Loss: 0.8902338, Training Acc: 0.000\n",
      "Epoch 6/25, Training Loss: 0.8889154, Training Acc: 0.000\n",
      "Epoch 7/25, Training Loss: 0.8874690, Training Acc: 0.000\n",
      "Epoch 8/25, Training Loss: 0.8858199, Training Acc: 0.000\n",
      "Epoch 9/25, Training Loss: 0.8839297, Training Acc: 0.000\n",
      "Epoch 10/25, Training Loss: 0.8817482, Training Acc: 0.000\n",
      "Epoch 11/25, Training Loss: 0.8793122, Training Acc: 0.000\n",
      "Epoch 12/25, Training Loss: 0.8765566, Training Acc: 0.000\n",
      "Epoch 13/25, Training Loss: 0.8734149, Training Acc: 0.000\n",
      "Epoch 14/25, Training Loss: 0.8698644, Training Acc: 0.000\n",
      "Epoch 15/25, Training Loss: 0.8659084, Training Acc: 0.000\n",
      "Epoch 16/25, Training Loss: 0.8618139, Training Acc: 0.000\n",
      "Epoch 17/25, Training Loss: 0.8572393, Training Acc: 0.000\n",
      "Epoch 18/25, Training Loss: 0.8526500, Training Acc: 0.034\n",
      "Epoch 19/25, Training Loss: 0.8480869, Training Acc: 0.405\n",
      "Epoch 20/25, Training Loss: 0.8433416, Training Acc: 1.014\n",
      "Epoch 21/25, Training Loss: 0.8386030, Training Acc: 1.351\n",
      "Epoch 22/25, Training Loss: 0.8340500, Training Acc: 2.061\n",
      "Epoch 23/25, Training Loss: 0.8292238, Training Acc: 2.162\n",
      "Epoch 24/25, Training Loss: 0.8242926, Training Acc: 2.669\n",
      "Epoch 25/25, Training Loss: 0.8192626, Training Acc: 2.804\n"
     ]
    }
   ],
   "source": [
    "#Training dan Validasi CNN\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "#Train\n",
    "\n",
    "for epoch in range(epoch_max):\n",
    "    benar = 0\n",
    "    salah = 0\n",
    "    iter_loss = 0.0\n",
    "    iterations = 0\n",
    "    model.train()\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        iter_loss = iter_loss+loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        outputs_recog = torch.round(outputs)\n",
    "        labels_recog = torch.round(labels)\n",
    "        for hitung in range(batch_size_train):\n",
    "            if torch.equal(outputs_recog[hitung,:], labels_recog[hitung,:]):\n",
    "                benar = benar + 1\n",
    "            else:\n",
    "                salah = salah + 1\n",
    "        iterations = iterations + 1\n",
    "                    \n",
    "    recognition_rate = (benar/Total_Train_Photo)*100\n",
    "    train_accuracy.append(recognition_rate)\n",
    "    train_loss.append(iter_loss)\n",
    "    \n",
    "    if (epoch+1)%1000 == 0:\n",
    "        torch.save(model.state_dict(),'CNN_3DFaceRecog_Save_5050_Row_v8.pth')\n",
    "    \n",
    "# Validasi \n",
    "    if (epoch+1)%100 == 0:\n",
    "        loss_iter_val = 0.0\n",
    "        benar_val = 0\n",
    "        salah_val = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss_v = loss_fn(outputs, labels)\n",
    "            loss_iter_val = loss_iter_val+loss_v.item()\n",
    "            outputs_recog = torch.round(outputs)\n",
    "            labels_recog = torch.round(labels)\n",
    "            for hitung in range(batch_size_val):\n",
    "                if torch.equal(outputs_recog[hitung,:], labels_recog[hitung,:]):\n",
    "                    benar_val = benar_val + 1\n",
    "                else:\n",
    "                    salah_val = salah_val + 1\n",
    "            iterations = iterations + 1\n",
    "\n",
    "        recognition_rate_v = (benar_val/Total_Val_Photo)*100\n",
    "        val_accuracy.append(recognition_rate_v)\n",
    "        val_loss.append(loss_iter_val)\n",
    "    \n",
    "    print ('Epoch {}/{}, Training Loss: {:.7f}, Training Acc: {:.3f}'.\n",
    "           format(epoch+1, epoch_max, \n",
    "                  train_loss[-1], \n",
    "                  train_accuracy[-1])); \n",
    "    if (epoch+1)%100 == 0:\n",
    "        print('\\nValidation || Validation Loss: {:.7f}, Validation Acc: {:.3f}\\n'\n",
    "              .format(val_loss[-1],\n",
    "                     val_accuracy[-1]))\n",
    "end = time.time()\n",
    "\n",
    "elapsed = end - start\n",
    "print('Elapased Time: ', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, 'b') # 'r' is the color red\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss_value')\n",
    "plt.title('Loss Graph Training')\n",
    "plt.show()\n",
    "print('Last epoch training loss: {}' .format(train_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'CNN_3DFaceRecog_Save_5050_Row_v8.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CNN_3DFaceRecognition_v8_5050_Row_TV')\n",
    "print('Epochs: {}, Training Loss: {:.7f}, Training Acc: {:.3f}, Validation Loss: {:.7f}, Validation Acc: {:.3f}'\n",
    "           .format(epoch+1,\n",
    "                   train_loss[-1],\n",
    "                   train_accuracy[-1],\n",
    "                   val_loss[-1],\n",
    "                   val_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.savemat('Epochs_and_time_5050R_v8.mat', {'Epochs':epoch, 'time':elapsed, 'train_loss':train_loss, 'train_accuracy':train_accuracy})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
