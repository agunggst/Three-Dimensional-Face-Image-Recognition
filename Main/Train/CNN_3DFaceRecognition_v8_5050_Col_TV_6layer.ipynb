{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "#%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('Train_Raw_C.mat')\n",
    "Train_Raw = mat['Train_Raw']\n",
    "mat1 = scipy.io.loadmat('Val_Raw_C.mat')\n",
    "Val_Raw = mat1['Val_Raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2850\n",
      "2700\n"
     ]
    }
   ],
   "source": [
    "l,w = 45,35\n",
    "Total_Train_Photo = int((Train_Raw.shape[0]*Train_Raw.shape[1])/(l*w))\n",
    "Total_Val_Photo = int((Val_Raw.shape[0]*Val_Raw.shape[1])/(l*w))\n",
    "Photo_Train_Col = 19\n",
    "Photo_Val_Col = 18\n",
    "print(Total_Train_Photo)\n",
    "print(Total_Val_Photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalisasi\n",
    "Train_Raw_Norm = Train_Raw/255\n",
    "Val_Raw_Norm = Val_Raw/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2850, 45, 35)\n",
      "(2700, 45, 35)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "input_data_raw = np.zeros([Total_Train_Photo,l,w])\n",
    "val_data_raw = np.zeros([Total_Val_Photo,l,w])\n",
    "for a in range(0,int(Total_Train_Photo/Photo_Train_Col)):\n",
    "    for b in range(0,Photo_Train_Col):\n",
    "        input_data_raw[i,:,:] = Train_Raw_Norm[45*a:45+45*a,35*b:35+35*b]\n",
    "        i=i+1\n",
    "i,a,b=0,0,0\n",
    "for a in range(0,int(Total_Val_Photo/Photo_Val_Col)):\n",
    "    for b in range(0,Photo_Val_Col):\n",
    "        val_data_raw[i,:,:] = Val_Raw_Norm[45*a:45+45*a,35*b:35+35*b]\n",
    "        i=i+1\n",
    "        \n",
    "print(input_data_raw.shape)\n",
    "print(val_data_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(input_data_raw[2849,:,:], cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2850, 10)\n",
      "(2700, 10)\n"
     ]
    }
   ],
   "source": [
    "tar = np.eye(10)\n",
    "target_train = np.zeros([Total_Train_Photo,10])\n",
    "target_val = np.zeros([Total_Val_Photo,10])\n",
    "for d in range(0,10):    \n",
    "    target_train[0+int(Total_Train_Photo/10)*d:int(Total_Train_Photo/10)+int(Total_Train_Photo/10)*d,:] = tar[d,:]\n",
    "d=0\n",
    "for d in range(0,10):\n",
    "    target_val[0+int(Total_Val_Photo/10)*d:int(Total_Val_Photo/10)+int(Total_Val_Photo/10)*d,:] = tar[d,:]\n",
    "print(target_train.shape)\n",
    "print(target_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tebal_train, tebal_val = Total_Train_Photo, Total_Val_Photo\n",
    "target_train3d = np.zeros(shape=(tebal_train, 1, 10))\n",
    "target_val3d = np.zeros(shape=(tebal_val, 1, 10))\n",
    "i=0\n",
    "\n",
    "for i in range(tebal_train):\n",
    "    target_train3d[i,0,:] = target_train[i,:]\n",
    "for i in range(tebal_val):\n",
    "    target_val3d[i,0,:] = target_val[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_in4d(data):\n",
    "    image_size = [45,35]\n",
    "    data4d = np.zeros([Total_Train_Photo,1,image_size[0],image_size[1]])\n",
    "    data4d[:,0,:,:] = data\n",
    "    return data4d\n",
    "\n",
    "def val_inl4d(data):\n",
    "    image_size = [45,35]\n",
    "    data4d = np.zeros([Total_Val_Photo,1,image_size[0],image_size[1]])\n",
    "    data4d[:,0,:,:] = data\n",
    "    return data4d\n",
    "\n",
    "def train_tar4d(data):\n",
    "    image_size = [1,10]\n",
    "    data4d = np.zeros([Total_Train_Photo,1,image_size[0],image_size[1]])\n",
    "    data4d[:,0,:,:] = data\n",
    "    return data4d\n",
    "\n",
    "def val_tarl4d(data):\n",
    "    image_size = [1,10]\n",
    "    data4d = np.zeros([Total_Val_Photo,1,image_size[0],image_size[1]])\n",
    "    data4d[:,0,:,:] = data\n",
    "    return data4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2850, 1, 45, 35)\n",
      "(2700, 1, 45, 35)\n",
      "(2850, 1, 1, 10)\n",
      "(2700, 1, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "input_train4d = train_in4d(input_data_raw)\n",
    "input_val4d = val_inl4d(val_data_raw)\n",
    "\n",
    "target_train4d = train_tar4d(target_train3d)\n",
    "target_val4d = val_tarl4d(target_val3d)\n",
    "\n",
    "print(input_train4d.shape)\n",
    "print(input_val4d.shape)\n",
    "print(target_train4d.shape)\n",
    "print(target_val4d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "epoch_max = 12000\n",
    "learning_rate = 0.2\n",
    "batch_size_train = int(Total_Train_Photo/10)\n",
    "batch_size_val = int(Total_Val_Photo/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2850 images in the training set\n",
      "There are 2700 images in the test set\n",
      "There are 10 batches in the train loader\n",
      "There are 10 batches in the testloader\n"
     ]
    }
   ],
   "source": [
    "input_train4d = torch.from_numpy(input_train4d).float().cuda()\n",
    "target_train4d = torch.from_numpy(target_train4d).float().cuda()\n",
    "input_val4d = torch.from_numpy(input_val4d).float().cuda()\n",
    "target_val4d = torch.from_numpy(target_val4d).float().cuda()\n",
    "\n",
    "target_train = torch.from_numpy(target_train).float().cuda()\n",
    "target_val = torch.from_numpy(target_val).float().cuda()\n",
    "\n",
    "dataset_train = torch.utils.data.TensorDataset(input_train4d, target_train)\n",
    "dataset_val = torch.utils.data.TensorDataset(input_val4d, target_val)\n",
    "\n",
    "kwargs = {}\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train,\n",
    "                                           batch_size=batch_size_train, \n",
    "                                           shuffle=True,**kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val,\n",
    "                                         batch_size=batch_size_val, \n",
    "                                         shuffle=True,**kwargs)\n",
    "\n",
    "print('There are {} images in the training set'.format(len(dataset_train)))\n",
    "print('There are {} images in the test set'.format(len(dataset_val)))\n",
    "print('There are {} batches in the train loader'.format(len(train_loader)))\n",
    "print('There are {} batches in the testloader'.format(len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, kernel_size=5, stride=1, padding=0),\n",
    "#             nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=1))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(20, 40, kernel_size=5, stride=1, padding=0),\n",
    "#             nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1))\n",
    "        self.fc1 = nn.Linear(32 * 22 * 40, 1000)\n",
    "#         self.drop_out = nn.Dropout()\n",
    "        self.act1 = nn.Tanh()\n",
    "    \n",
    "        self.fc2 = nn.Linear(1000, 500)\n",
    "        self.act2 = nn.Tanh()\n",
    "        \n",
    "        self.fc3 = nn.Linear(500,10)\n",
    "        self.act3 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "#         out = self.drop_out(out)\n",
    "        out = out.view(-1, 32 * 22 * 40)\n",
    "        out = self.act1(self.fc1(out))\n",
    "#         out = self.fc1(out)\n",
    "        out = self.act2(self.fc2(out))\n",
    "        out = self.act3(self.fc3(out))\n",
    "#         out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=4, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(20, 40, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=28160, out_features=1000, bias=True)\n",
      "  (act1): Tanh()\n",
      "  (fc2): Linear(in_features=1000, out_features=500, bias=True)\n",
      "  (act2): Tanh()\n",
      "  (fc3): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (act3): Sigmoid()\n",
      ")\n",
      "torch.Size([20, 1, 5, 5])\n",
      "torch.Size([20])\n",
      "torch.Size([40, 20, 5, 5])\n",
      "torch.Size([40])\n",
      "torch.Size([1000, 28160])\n",
      "torch.Size([1000])\n",
      "torch.Size([500, 1000])\n",
      "torch.Size([500])\n",
      "torch.Size([10, 500])\n",
      "torch.Size([10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "model = model.cuda()    \n",
    "loss_fn = torch.nn.MSELoss(size_average='false').cuda()        \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.3)\n",
    "print(model)\n",
    "for p in model.parameters():\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12, Training Loss: 1.3881382, Training Acc: 0.000\n",
      "Epoch 2/12, Training Loss: 0.9027035, Training Acc: 0.000\n",
      "Epoch 3/12, Training Loss: 0.8985813, Training Acc: 0.000\n",
      "Epoch 4/12, Training Loss: 0.8951296, Training Acc: 0.000\n",
      "Epoch 5/12, Training Loss: 0.8916807, Training Acc: 0.000\n",
      "Epoch 6/12, Training Loss: 0.8878553, Training Acc: 0.000\n",
      "Epoch 7/12, Training Loss: 0.8834653, Training Acc: 0.000\n",
      "Epoch 8/12, Training Loss: 0.8786623, Training Acc: 0.000\n",
      "Epoch 9/12, Training Loss: 0.8731781, Training Acc: 0.000\n",
      "Epoch 10/12, Training Loss: 0.8675051, Training Acc: 0.000\n",
      "Epoch 11/12, Training Loss: 0.8613567, Training Acc: 0.211\n",
      "Epoch 12/12, Training Loss: 0.8558326, Training Acc: 1.053\n"
     ]
    }
   ],
   "source": [
    "#Training dan Validasi CNN\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "#Train\n",
    "\n",
    "for epoch in range(epoch_max):\n",
    "    benar = 0\n",
    "    salah = 0\n",
    "    iter_loss = 0.0\n",
    "    iterations = 0\n",
    "    model.train()\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        iter_loss = iter_loss+loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        outputs_recog = torch.round(outputs)\n",
    "        labels_recog = torch.round(labels)\n",
    "        for hitung in range(batch_size_train):\n",
    "            if torch.equal(outputs_recog[hitung,:], labels_recog[hitung,:]):\n",
    "                benar = benar + 1\n",
    "            else:\n",
    "                salah = salah + 1\n",
    "        iterations = iterations + 1\n",
    "                    \n",
    "    recognition_rate = (benar/Total_Train_Photo)*100\n",
    "    train_accuracy.append(recognition_rate)\n",
    "    train_loss.append(iter_loss)\n",
    "    \n",
    "    if (epoch+1)%1000 == 0:\n",
    "        torch.save(model.state_dict(),'CNN_3DFaceRecog_Save_5050_Col_v8_6layer.pth')\n",
    "    \n",
    "#Validasi    \n",
    "    if (epoch+1)%100 == 0:\n",
    "        loss_iter_val = 0.0\n",
    "        benar_val = 0\n",
    "        salah_val = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss_v = loss_fn(outputs, labels)\n",
    "            loss_iter_val = loss_iter_val+loss_v.item()\n",
    "            outputs_recog = torch.round(outputs)\n",
    "            labels_recog = torch.round(labels)\n",
    "            for hitung in range(batch_size_val):\n",
    "                if torch.equal(outputs_recog[hitung,:], labels_recog[hitung,:]):\n",
    "                    benar_val = benar_val + 1\n",
    "                else:\n",
    "                    salah_val = salah_val + 1\n",
    "            iterations = iterations + 1\n",
    "\n",
    "        recognition_rate_v = (benar_val/Total_Val_Photo)*100\n",
    "        val_accuracy.append(recognition_rate_v)\n",
    "        val_loss.append(loss_iter_val)\n",
    "    \n",
    "    print ('Epoch {}/{}, Training Loss: {:.7f}, Training Acc: {:.3f}'.\n",
    "           format(epoch+1, epoch_max, \n",
    "                  train_loss[-1], \n",
    "                  train_accuracy[-1])); \n",
    "    if (epoch+1)%100 == 0:\n",
    "        print('\\nValidation || Validation Loss: {:.7f}, Validation Acc: {:.3f}\\n'\n",
    "              .format(val_loss[-1],\n",
    "                     val_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch validation loss: 0.8558326363563538\n"
     ]
    }
   ],
   "source": [
    "plt.plot(train_loss, 'b') # 'r' is the color red\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss_value')\n",
    "plt.title('Loss Graph Training')\n",
    "plt.show()\n",
    "print('Last epoch training loss: {}' .format(train_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'CNN_3DFaceRecog_Save_5050_Col_v8_6layer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_3DFaceRecognition_v7_5050_Col_TV\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-5ca8ae165ef0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                    \u001b[0mtrain_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                    \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                    \u001b[0mval_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m                    val_accuracy[-1]))\n\u001b[0;32m      8\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('CNN_3DFaceRecognition_v8_5050_Col_TV_6layer')\n",
    "print('Epochs: {}, Training Loss: {:.7f}, Training Acc: {:.3f}, Validation Loss: {:.7f}, Validation Acc: {:.3f}'\n",
    "           .format(epoch+1,\n",
    "                   train_loss[-1],\n",
    "                   train_accuracy[-1],\n",
    "                   val_loss[-1],\n",
    "                   val_accuracy[-1]))\n",
    "end = time.time()\n",
    "\n",
    "elapsed = end - start\n",
    "print('Elapased Time: ', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.savemat('Epochs_time_loss_5050C_v8.mat', {'Epochs':epoch, 'time':elapsed, 'train_loss':train_loss, 'train_accuracy':train_accuracy})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
